

CUDA:
  # desc: torch.backends
  cudnn: true

LOG:
  log_every_n_updates: 20
  eval_interval: 5000
  num_eval_episodes: 10

WANDB:
  # desc: wandb.watch function inputs
  log_param: true
  log_type: gradients
  log_freq: 1000

ENV:
  # Choice: [clip_act, vec_norm, norm_clip_obs, absorbing]
  wrapper: [clip_act, filter_obs, flatten_obs, time_aware, done_success]  # default: [] (no wrapper)

TEST_ENV:
  wrapper: [clip_act, filter_obs, flatten_obs, time_aware, done_success]  # default: [] (no wrapper)


OPTIM:
  # desc: Optimizer class and zero_grad input
  optim_cls: adam        # default: adam
  optim_set_to_none: true  # default: false
  # weight_decay: 10       # default: 0.0


ALGO:
  max_grad_norm: 10  # default: None 10 is good
  gamma: 0.99         # default: 0.99
  seed: 0


PPO:
  # desc: PPO unique inputs
  epoch_ppo: 20     # default: 10
  gae_lambda: 0.97  # default: 0.97
  clip_eps: 0.2     # default: 0.2
  coef_ent: 0.01   # default: 0.0

  with_reward: false  # dont use reward for ail 

  # PPO policy_kwargs
  pi: [32, 32]              # default: [64, 64]
  vf: [256, 256]              # default: [64, 64]
  activation: relu_inplace  # default: "relu"
  lr_actor: 3.0e-4            # default: 3.0e-4 
  lr_critic: 3.0e-4           # default: 3.0e-4
  # orthogonal_init: true       # default: false


SAC:
  # desc: SAC unique inputs
  start_steps: 10_000
  num_gradient_steps: 1       # deafult: 1 # ! slow O(n)
  target_update_interval: 1   # deafult: 1
  tau: 0.005                   # default: 0.005
  log_alpha_init: 1.0         # default: 1.0
  lr_alpha: 3.0e-4            # default: 3.0e-4

  # batch_size: 256             # default: 256        
  buffer_size: 3_000_000      # default: 1_000_000
  with_reward: false          # dont use reward for ail
  extra_data: ["log_pis", "remaining_steps"]              # default: []
  
  # SAC policy_kwargs
  # pi: [128, 128]              # default: [128, 128]
  # qf: [128, 128]              # default: [128, 128]
  pi: [512, 512]              # default: [128, 128]
  qf: [512, 512]              # default: [128, 128]
  activation: "relu_inplace"  # default: "relu"
  lr_actor: 3.0e-4            # default: 7.3e-4
  lr_critic: 5.0e-4           # default: 7.3e-4



# Discriminator settings
DISC:
  # use_spectral_norm: true    # default: false
  # dropout_input: true        # default: false
  # dropout_input_rate: 0.5     # default: 0.1
  # dropout_hidden: true       # default: false
  # dropout_hidden_rate: 0.75   # default: 0.1
  
  # Discriminator Architecture
  hidden_units: [512]    # default: [128, 128] 
  hidden_activation: relu_inplace

  # ent_coef: 0.03  # default: 0.0

  epoch_disc: 1  
  lr_disc: 7.0e-5
  # choice = ["logsigmoid", "softplus", "logit"] # default: "logsigmoid"
  rew_input_choice: logsigmoid
  # rew_input_choice: softplus
  # rew_input_choice: logit

  # rew_clip: true         # default: False
  # max_rew_magnitude: 1.0   # default: 10.0
  # min_rew_magnitude: -1.0  # default: -10.0

  # infinite_horizon: true     # default: false
    

AIRL:
  disc_cls: airl_sa  
  # subtract_logp: true   # default: false


GAIL:
  inverse: false   # default: false
  # inverse: true  # default: false


# Record:
# InvertedPendulum:
#   airl_so: # * work without subtract_logp, unstable otherwise
      # logsigmoid:
      # softplus:
      # logit:
#   airl_sa: # * work without subtract_logp, unstable otherwise
      # logsigmoid: pass
      # softplus: pass
      # logit: pass
#   gail: work
      # logsigmoid: pass
      # softplus: pass


# HalfCheetah:
#   airl_so:
#   airl_sa:
#   gail:

# Hopper: 
#   desc:
      # ppo: pi_lr 7.3e-4, vf_lr 3.0e-4 disc_lr 1.0e-4
#   airl_so:
#   airl_sa:
#   gail:
